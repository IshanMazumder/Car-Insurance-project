# -*- coding: utf-8 -*-
"""MLProject_Insurance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VluO5ITpNMVxUnybKiJl3fD4gyA5W_LZ

# Load Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
from sklearn import metrics
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt  
from sklearn.preprocessing import StandardScaler
from xgboost.sklearn import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.linear_model import LogisticRegression
#from sklearn.neighbors import NeighborhoodComponentAnalysis
from sklearn.decomposition import PCA

"""# Reading unclean data"""

d = pd.read_csv('D:\\ishan\\pythonproject\\Final project\\Insurence\\train2.csv', na_values = [-1])

"""missing values"""

# missing values - ps_ind_02_cat,ps_ind_04_cat,ps_ind_05_cat,ps_reg_03,ps_car_01_cat,ps_car_03_cat(411231),ps_car_05_cat(266551)
#ps_car_07_cat,ps_car_09_cat,ps_car_11,ps_car_14

"""co relation between interval variables"""

col1 = ['ps_ind_01','ps_ind_03','ps_ind_14','ps_ind_15','ps_reg_01','ps_reg_02','ps_reg_03','ps_car_11','ps_car_12','ps_car_13','ps_car_14','ps_car_15','ps_calc_01','ps_calc_02','ps_calc_03','ps_calc_04','ps_calc_05','ps_calc_06','ps_calc_07','ps_calc_08','ps_calc_09','ps_calc_10','ps_calc_11','ps_calc_12','ps_calc_13','ps_calc_14','ps_calc_15']
l = len(internal_array[0])
for i in range (0,l):
    m = internal_array[0][i]
    n = internal_array[1][i]
    if not col1[m] == col1[n]:
        print((col1[m],col1[n])) # co relation between interval variables

"""co relation between ordinal variables"""

col2 = ['ps_ind_02_cat','ps_ind_04_cat','ps_ind_08_cat','ps_car_01_cat','ps_car_02_cat','ps_car_03_cat','ps_car_04_cat','ps_car_05_cat','ps_car_06_cat','ps_car_07_cat','ps_car_08_cat','ps_car_09_cat','ps_car_10_cat','ps_car_11_cat']
l = len(ordinal_array[0])
for i in range (0,l):
    m = ordinal_array[0][i]
    n = ordinal_array[1][i]
    if not col2[m] == col2[n]:
        print((col2[m],col2[n])) # co relation between ordinal variables

continuous=df.dtypes[(df.dtypes=='int64')|(df.dtypes == 'float64')].index

fig=plt.figure(figsize=(20,15))
for i in range(0,len(continuous)):
    ax=fig.add_subplot(6,10,i+1)
    sns.boxplot(y=df[continuous[i]])
    ax.set_title(df.columns[i],color='Blue')
    plt.grid()
plt.tight_layout()

"""Outliters detection
![alt text](https://drive.google.com/uc?export=view&id=1Fz5AzfF6cXRt53u-aZo50BpeBq0fwof4)
"""



"""Corelation matrix"""

corr_matrix = corr_matrix.iloc[:,:].astype(np.float64)
corr_matrix = pd.DataFrame(columns = data.columns , index = data.columns)
for i in range(0,59):
    #print(col[i])
    for j in range(0,59):
        #print(np.corrcoef(data.iloc[:,i:(i+1)],data.iloc[:,j:(j+1)],rowvar =0)[0][1])
        corr_matrix.iloc[j:(j+1),i:(i+1)]= np.corrcoef(data.iloc[:,i:(i+1)],data.iloc[:,j:(j+1)],rowvar =0)[0][1]

"""#Load file from Google Drive"""

from google.colab import drive
drive.mount('/content/drive')
path = '/content/drive/My Drive/Colab Notebooks/'

"""# Read Clean CSV 

---



1.   Imputed missing values
2.   Dropped two features with heighest missing data
3.   Treated Outliers
4.   Checked for duplicate data rows not present
"""

df = pd.read_csv(path+'/clean_data.csv')
df = df.drop('id', axis =1)

df.head()

df.shape

"""# Scaling - df"""

stdScaler = StandardScaler()
c = ['ps_ind_01','ps_ind_03','ps_ind_14','ps_ind_15','ps_reg_02','ps_reg_03','ps_car_11','ps_car_13','ps_car_15','ps_calc_04','ps_calc_05','ps_calc_06','ps_calc_07','ps_calc_08','ps_calc_09','ps_calc_10','ps_calc_11','ps_calc_12','ps_calc_13','ps_calc_14']

df.loc[:,c] = stdScaler.fit_transform(df.loc[:,c])

df.head()

#xx_train_data_smote = pd.DataFrame(xx_train_data_smote, columns = colx)
#yy_train_data_smote = pd.DataFrame(y_train_scaled_data, columns = ['target'])

"""# split in X and Y"""

X = df.iloc[:,1:]
Y = df.iloc[:, :1]

"""# Feature engineering

---

selecting top 20 cols
"""

cols = ['target','ps_car_13','ps_ind_03','ps_car_14','ps_reg_03','ps_ind_05_cat','ps_reg_01','ps_ind_01','ps_ind_15','ps_car_01_cat','ps_car_11_cat','ps_car_09_cat','ps_ind_17_bin','ps_reg_02','ps_calc_14','ps_car_07_cat','ps_car_15','ps_car_06_cat','ps_car_11','ps_ind_06_bin','ps_calc_05','ps_calc_10','ps_car_12','ps_calc_06','ps_ind_16_bin','ps_calc_03','ps_ind_09_bin','ps_calc_11','ps_calc_04']#,'ps_calc_08','ps_calc_01','ps_car_04_cat','ps_calc_13','ps_ind_07_bin']
c = df.columns

for cc in c:
  if cc not in cols:
    df.drop(columns=cc, axis=1, inplace=True)

df.shape

"""# One Hot Encoding"""

c = ['ps_ind_04_cat','ps_ind_05_cat','ps_ind_02_cat','ps_car_01_cat','ps_car_02_cat','ps_car_04_cat','ps_car_06_cat','ps_car_07_cat','ps_car_08_cat','ps_car_09_cat','ps_car_10_cat']

df_1hot = pd.get_dummies(df, columns= c)

df_1hot.shape

"""#Set 1-  Test/Train Data
---

for good "1" prediction- x_train, x_test, y_train, y_test
"""

df_zeros = df[df.target == 0] 
df_ones = df[df.target == 1]
x_0 = df_zeros.iloc[:,1:]
x_1 = df_ones.iloc[:,1:]
y_0 = df_zeros.iloc[:,0:1]
y_1 = df_ones.iloc[:,0:1]

x_0_train, x_0_test, y_0_train, y_0_test = train_test_split(x_0, y_0, test_size=0.99, random_state=42)

x_1_train, x_1_test, y_1_train, y_1_test = train_test_split(x_1, y_1, test_size=0.5, random_state=42)

train_0 = pd.DataFrame(x_0_train)
train_0['target'] = y_0_train

train_1 = pd.DataFrame(x_1_train)
train_1['target'] = y_1_train

df_train = train_1.append(train_0)

test_0 = pd.DataFrame(x_0_test)
test_0['target'] = y_0_test

test_1 = pd.DataFrame(x_1_test)
test_1['target'] = y_1_test

df_test = test_1.append(test_0)

x_train = df_train.iloc[:,0:55]
y_train = df_train.iloc[:,55:]

x_test = df_test.iloc[:,0:55]
y_test = df_test.iloc[:,55:]

x_train = df_train.iloc[:,0:28]
y_train = df_train.iloc[:,28:]

x_test = df_test.iloc[:,0:28]
y_test = df_test.iloc[:,28:]

x_0_train.shape

y_train.head()

"""# Set 2 -Test Data/Train data(balanced)

---
 x_test, x_train, y_test, y_train
"""

# seperate 'o' and '1'

df_zeros = df[df.target == 0] 
df_ones = df[df.target == 1]

# df_zeros_test, df_zeros_train

df_zeros.shape

from sklearn.utils import shuffle
df_zeros = shuffle(df_zeros)
df_zeros_train = df_zeros.iloc[:286759,:]
df_zeros_test = df_zeros.iloc[286759:,:]

df_zeros_train.shape

from sklearn.utils import shuffle
df_ones = shuffle(df_ones)
df_ones_train = df_ones.iloc[:10847,:]
df_ones_test = df_ones.iloc[10847:,:]

df_ones_test.shape

df_train = df_ones_train.append(df_zeros_train)
df_test = df_ones_test.append(df_zeros_test)

df_train.head() #ok? -yes

# devide data into X and Y

x_train = df_train.iloc[:,1:]
y_train = df_train.iloc[:,:1]

x_test = df_test.iloc[:,1:]
y_test = df_test.iloc[:,:1]

"""# data for Q8- x_test, x_train, y_test, y_train"""

# seperate 'o' and '1'

df_zeros = df[df.target == 0] 
df_ones = df[df.target == 1]

# df_zeros_test, df_zeros_train

df_zeros.shape

from sklearn.utils import shuffle
df_zeros = shuffle(df_zeros)
df_zeros_train = df_zeros.iloc[:70000,:]
df_zeros_test = df_zeros.iloc[70000:,:]

df_zeros_train.shape

from sklearn.utils import shuffle
df_ones = shuffle(df_ones)
df_ones_train = df_ones.iloc[:10847,:]
df_ones_test = df_ones.iloc[10847:,:]

df_ones_test.shape

df_train = df_ones_train.append(df_zeros_train)
df_test = df_ones_test.append(df_zeros_test)

df_train.head() #ok? -yes

# devide data into X and Y

x_train = df_train.iloc[:,1:]
y_train = df_train.iloc[:,:1]

x_test = df_test.iloc[:,1:]
y_test = df_test.iloc[:,:1]

"""#Set 3 -Test Data/Train data(balanced)

---
1hot/ x_test, x_train, y_test, y_train
"""

# seperate 'o' and '1'

df_zeros = df_1hot[df_1hot.target == 0] 
df_ones = df_1hot[df_1hot.target == 1]

# df_zeros_test, df_zeros_train

df_zeros.shape

from sklearn.utils import shuffle
df_zeros = shuffle(df_zeros)
df_zeros_train = df_zeros.iloc[:5500,:]
df_zeros_test = df_zeros.iloc[5500:,:]

df_zeros_train.shape

from sklearn.utils import shuffle
df_ones = shuffle(df_ones)
df_ones_train = df_ones.iloc[:10847,:]
df_ones_test = df_ones.iloc[10847:,:]

df_ones_test.shape

df_train = df_ones_train.append(df_zeros_train)
df_test = df_ones_test.append(df_zeros_test)

df_train.head() #ok? -yes

# devide data into X and Y

x_train = df_train.iloc[:,1:]
y_train = df_train.iloc[:,:1]

x_test = df_test.iloc[:,1:]
y_test = df_test.iloc[:,:1]

"""# SMOTE and Upsample Code

---
for scenario to predict "1" better/ x_train_smote, y_train_smote
"""

over = SMOTE(sampling_strategy=0.8) 
x_train_smote , y_train_smote = over.fit_sample(x_train, y_train)

#over_l = SMOTE(sampling_strategy=0.3) # lets do magic now
#x_train_data_smote, y_train_data_smote = over_l.fit_sample(x_train_data, y_train_data)
from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=42,sampling_strategy=0.39)
x_train_smote, y_train_smote = ros.fit_resample(x_train, y_train)
#x_train_data_smote, y_train_data_smote = x_train_data, y_train_data

colx = x_train.columns
x_train_smote = pd.DataFrame(x_train_smote, columns=colx)

y_train_smote = pd.DataFrame(y_train_smote, columns =['target'])

new_data = y_train_smote.join(x_train_smote)

df_0 = new_data[new_data.target == 0]
df_1 = new_data[new_data.target == 1]

print(df_0.shape)
print(df_1.shape)

x_train = x_train_smote
y_train = y_train_smote

"""# duplicate more 1s"""

xtra = x_1_train.iloc[:5000,:55]
ytra = y_1_train.iloc[:5000,:]

x_train = x_train.append(xtra)
y_train = y_train.append(ytra)

ytra.head()

"""# PCA - x_train, x_test"""

pca = PCA(n_components = 30)

x_train = pca.fit_transform(x_train)

x_test = pca.fit_transform(x_test)

"""# Grid Search for multiplr algorithm!"""

from xgboost.sklearn import XGBClassifier
from sklearn.model_selection import GridSearchCV

n_layer_sizes=(100,100,150),activation= 'relu', max_iter=5000, solver='sgd', verbose=True,  random_state=21,tol=0.001, learning_rate_init=0.03

weights = np.linspace(0.3, 0.8, 5)
gsc = GridSearchCV(
    estimator=LinearSVC(dual = False),
    param_grid={
     
      'penalty' :['l2'],
      'loss': ['squared_hinge'],
      'max_iter' :[200,500,1000]

    },
    scoring='f1',
    cv=3
)

grid_result = gsc.fit(x_train, y_train)
print("Best parameters : %s" % grid_result.best_params_)

"""# Logistics regression"""

lr = LogisticRegression() # use class weight after grid search

parameters = lr.fit(x_train,y_train)

y_pred_lr = lr.predict(x_test)

con_metric = pd.DataFrame(metrics.confusion_matrix(y_test,y_pred_lr), index = ['acctual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])
con_metric

print(metrics.classification_report(y_test,y_pred_lr))

x_values = [np.min(X[:, 1] - 5), np.max(X[:, 2] + 5)]
y_values = - (parameters[0] + np.dot(parameters[1], x_values)) / parameters[2]

plt.plot(x_values, y_values, label='Decision Boundary')
plt.xlabel('Marks in 1st Exam')
plt.ylabel('Marks in 2nd Exam')
plt.legend()
plt.show()

"""# SVM"""

from sklearn.svm import LinearSVC

svm = LinearSVC(max_iter=200,dual=True,)

svm.fit(x_train,y_train)

y_pred_svm = svm.predict(x_test)

print(metrics.classification_report(y_test,y_pred_svm))

con_metric = pd.DataFrame(metrics.confusion_matrix(y_test,y_pred_svm), index = ['acctual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])
con_metric

"""# **XG Boost**"""

from xgboost.sklearn import XGBClassifier #class_weight= {0: 0.3, 1: 0.7},

XG  = XGBClassifier( n_estimators = 100, scale_pos_weight= 1,class_weight= {0: 0.3, 1: 0.7})
XG.fit(x_train, y_train)

y_pred_XG =XG.predict(x_test)

print(metrics.classification_report(y_test,y_pred_XG))

con_metric = pd.DataFrame(metrics.confusion_matrix(y_test,y_pred_XG), index = ['acctual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])
con_metric

proba = XG.predict_proba(x_test)
proba = proba[:,0]

y_proba = np.where((proba <= 0.50),1,0)

con_metric = pd.DataFrame(metrics.confusion_matrix(y_test,y_proba), index = ['acctual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])
con_metric

"""# **Ada boost**"""

from sklearn.ensemble import AdaBoostClassifier

clf = AdaBoostClassifier(n_estimators=300, random_state=42)

clf.fit(x_train, y_train)

y_pred = clf.predict(x_test)

con_metric = pd.DataFrame(metrics.confusion_matrix(y_test,y_pred), index = ['acctual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])
con_metric

print(metrics.classification_report(y_test,y_pred.round()))

prob = clf.predict_proba(x_test)
y_prob = prob[:,1]

y_prob = np.where((y_prob <= 0.5),1,0)

con_metric = pd.DataFrame(metrics.confusion_matrix(y_test,y_proba), index = ['acctual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])
con_metric

"""# **Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

#Create a Gaussian Classifier
clfRF=RandomForestClassifier(n_estimators=100,  )#class_weight= {0: 0.3, 1: 0.7}

#con_metric = pd.DataFrame(metrics.confusion_matrix(y_test_m,y_pred_RF), index = ['acctual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])

clfRF.fit(x_train,y_train)

y_pred_RF=clfRF.predict(x_test) # zFP- ac= 1 pred- 0

con_metric = pd.DataFrame(metrics.confusion_matrix(y_test,y_pred_RF), index = ['acctual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])
con_metric

print(metrics.classification_report(y_test,y_pred_RF.round()))

"""# Keras"""

import keras

# find input dimentions - number of features in x

x_train_smote.shape[1]

from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# define the keras model
model = Sequential()
model.add(Dense(224, input_dim=x_train.shape[1], activation='relu'))
model.add(Dense(448, activation='relu'))
#model.add(Dropout(0.3))
model.add(Dense(800, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_train,y_train, epochs=50, batch_size=500)

y_pred_keras = model.predict_classes(x_test)

con_metric = pd.DataFrame(metrics.confusion_matrix(y_test,y_pred_keras), index = ['acctual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])
con_metric

print(metrics.classification_report(y_test,y_pred_keras.round()))

"""# **Artificial** **Neural** **Network**"""

from sklearn.neural_network import MLPClassifier

#'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 150), 'learning_rate': 'adaptive', 'solver': 'sgd'}

NN = MLPClassifier(hidden_layer_sizes=(50,100,150),activation= 'relu', max_iter=5000, alpha=0.05,solver='sgd', verbose=True,  random_state=21,tol=0.001, learning_rate='adaptive')

NN.fit(x_train,y_train)

y_pred_NN = NN.predict(x_test)

con_metric = pd.DataFrame(metrics.confusion_matrix(y_test,y_pred_NN), index = ['acctual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])
con_metric

print(metrics.classification_report(y_test,y_pred_NN.round()))



"""# LightGBM"""

from lightgbm import LGBMClassifier, plot_importance
import xgboost as xgb

#class_weight_lgb={0: 0.5, 1: 0.7}
lgb = LGBMClassifier(n_estimators=100,max_depth=7,learning_rate=0.01,random_state=42)
lgb.fit(x_train,y_train)
y_pred_light = lgb.predict(x_test)
print(metrics.classification_report(y_test,y_pred_light))

con_metric = pd.DataFrame(metrics.confusion_matrix(y_test,y_pred_light), index = ['acctual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])
con_metric

xgb.plot_importance(XG)
plt.rcParams['figure.figsize'] = [30, 30]
plt.show()

df.ps_calc_19_bin, df.ps_calc_15_bin,df.ps_ind_08_bin,df.ps_calc_17_bin,df.ps_calc_12, df.ps_ind_18_bin,df.ps_calc_09,df.ps_calc_18_bin, df.ps_ind_02_cat,df.ps_ind_04_cat, df.ps_calc_02,df.ps_calc_07,df.ps_ind_07_bin, df.ps_calc_13,

"""# catboost"""

#pip install catboost

from catboost import CatBoostClassifier

#class_weight_cb=[0.5, 0.6]
cb = CatBoostClassifier(iterations=100,learning_rate=0.001,  depth=5,eval_metric='F1')
# Fit model
cb.fit(x_train,y_train)
y_pred_cat = cb.predict(x_test)
print(metrics.classification_report(y_test,y_pred_cat))

con_metric = pd.DataFrame(metrics.confusion_matrix(y_test,y_pred_cat), index = ['acctual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])
con_metric